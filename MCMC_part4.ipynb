{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54a753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Import Required Packages\n",
    "# ========================\n",
    "import os, sys\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import casadi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "from gams import *\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"mcmc\"))\n",
    "from mcmc_sampling import create_hmc_sampler\n",
    "\n",
    "# Local Debugging flag; remove when all tested\n",
    "_DEBUG = True \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a321f9ca-bb8b-47e0-9017-cd2e906bb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gams import GamsWorkspace\n",
    "if GamsWorkspace.api_major_rel_number<42:  # old API structure\n",
    "    import gdxcc as gdx\n",
    "    from gams import *\n",
    "    import gamstransfer as gt\n",
    "else:  # new API structure\n",
    "    import gams.core.gdx as gdx\n",
    "    from gams.control import *\n",
    "    import gams.transfer as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a650f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal_log_probability(z, mean, cov):\n",
    "    \"\"\"\n",
    "    Evaluate and return the log-probability of a (unnormalized) multivariate normal distribution.\n",
    "\n",
    "    :param z: value of the multivariate normal random variable\n",
    "    :param mean: (1d array/list); mean of the normal distribution\n",
    "    :param cov: (2D array) covariance matrix\n",
    "\n",
    "    :returns: log-probability of a (unnormalized) multivariate normal distribution\n",
    "    \"\"\"\n",
    "    z    = np.asarray(z, dtype=float).flatten()\n",
    "    mean = np.asarray(z, dtype=float).flatten()\n",
    "    cov = np.asarray(cov, dtype=float).squeeze()\n",
    "\n",
    "    # Assert/Check input types/shapes\n",
    "    assert mean.size == z.size, \"mismatch between variable and mean sizes/shapes\"\n",
    "    if mean.size > 1:\n",
    "        assert cov.shape == (z.size,z.size), \"mismatch between variable and covariances `cov` shape\"\n",
    "\n",
    "    elif mean.size == 1:\n",
    "        assert cov.size == 1, \"mismatch between variable and covariances `cov` shape\"\n",
    "        cov = np.reshape(cov, (1, 1))\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid covariance matrix shape!\")\n",
    "        raise AssertionError\n",
    "\n",
    "    # Evaluate the log-probability\n",
    "    dev      = z - mean\n",
    "    scld_dev = np.linalg.inv(cov).dot(dev)\n",
    "    log_prob = - 0.5 * np.dot(dev, scld_dev)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23697e0d-7eda-405b-9dcd-0b7f5449df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_num = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f5da72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_site_data(site_num, \n",
    "                   norm_fac=1e6,\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Load site data\n",
    "\n",
    "    :returns:\n",
    "        -\n",
    "    \"\"\"\n",
    "    # Read data file\n",
    "    n=site_num\n",
    "    file=f\"data/calibration_{n}SitesModel.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    \n",
    "    # Extract information\n",
    "    z_2017             = df[f'z_2017_{n}Sites'].to_numpy()\n",
    "    zbar_2017          = df[f'zbar_2017_{n}Sites'].to_numpy()\n",
    "    gamma              = df[f'gamma_{n}Sites'].to_numpy()\n",
    "    gammaSD            = df[f'gammaSD_{n}Sites'].to_numpy()\n",
    "    forestArea_2017_ha = df[f'forestArea_2017_ha_{n}Sites'].to_numpy()\n",
    "    theta              = df[f'theta_{n}Sites'].to_numpy()\n",
    "\n",
    "    # Normalize Z data\n",
    "    zbar_2017 /= norm_fac\n",
    "    z_2017    /= norm_fac\n",
    "    \n",
    "    return (zbar_2017,\n",
    "            gamma,\n",
    "            gammaSD,\n",
    "            z_2017,\n",
    "            forestArea_2017_ha,\n",
    "            theta\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06da816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma [516.88557127 488.95668786 535.7933372  614.38476229 697.34085963\n",
      " 594.38440765 540.18969668 502.09011583 586.13789618 622.92870153\n",
      " 468.53543538 216.29945459 736.1126944  644.37719596 568.13204882\n",
      " 618.56342492 570.93769461 376.7589174  806.29456683 649.96115287\n",
      " 577.10631912 455.99483831 366.09394675 337.21852297 287.82630347]\n",
      "gammaSD [22.01380318 12.30650843 23.34479384 32.19887999 33.0552216  20.86507363\n",
      " 10.44879734  7.97210675 16.35650086 18.89356099  5.66673842  4.7426229\n",
      " 27.52295223 16.35653107 10.55158851 21.85866557 24.68789648  7.04434639\n",
      " 35.21752443 25.01272143 13.69821009  6.68635044  1.78628409 14.31382086\n",
      " 17.20154406]\n",
      "z_2017 [5.77092940e-07 5.58918518e-06 7.87674083e-04 9.64686174e-06\n",
      " 1.28634503e-04 1.62486537e-05 7.71270596e-05 4.13615376e-04\n",
      " 1.41563731e-03 2.97549040e-03 7.91141025e-03 3.11951500e-04\n",
      " 4.91859804e-04 5.48889538e-04 1.19367249e-03 1.41661798e-03\n",
      " 6.05514368e-03 6.74644202e-03 8.44104902e-05 2.44820053e-03\n",
      " 7.38590342e-03 8.45649924e-03 5.22554843e-03 8.60690505e-04\n",
      " 2.31193733e-03]\n",
      "gamma - gammaSD [494.87176808 476.65017943 512.44854336 582.1858823  664.28563803\n",
      " 573.51933402 529.74089934 494.11800908 569.78139532 604.03514054\n",
      " 462.86869696 211.55683169 708.58974217 628.02066489 557.58046031\n",
      " 596.70475936 546.24979813 369.71457101 771.0770424  624.94843144\n",
      " 563.40810902 449.30848787 364.30766265 322.90470211 270.62475941]\n",
      "zbar_2017 [0.00082784 0.00592396 0.0162693  0.00831711 0.01149272 0.00364199\n",
      " 0.02794187 0.02603153 0.02649412 0.02588343 0.01898131 0.00098645\n",
      " 0.02398373 0.02837916 0.02750545 0.0280807  0.0275304  0.00982846\n",
      " 0.00437791 0.00834653 0.0212187  0.02077097 0.01650749 0.00157716\n",
      " 0.00345833]\n",
      "theta [0.00973847 0.00974059 0.14542111 1.2232699  1.43857433 0.41013763\n",
      " 0.60516123 1.12924934 1.68193866 1.95604357 1.73093124 1.03977945\n",
      " 1.4751191  1.48204373 2.1503376  1.88858176 1.922081   2.11768005\n",
      " 2.22692684 2.38202071 2.98371336 2.60655349 2.12050328 2.70196986\n",
      " 2.66280855]\n"
     ]
    }
   ],
   "source": [
    "# Verbose Data check (eyeball check!)\n",
    "zbar_2017, gamma, gammaSD, \\\n",
    "            z_2017, forestArea_2017_ha, theta = load_site_data(site_num,norm_fac=1e+9)\n",
    "print(\"gamma\", gamma)\n",
    "print(\"gammaSD\", gammaSD)\n",
    "print(\"z_2017\", z_2017)\n",
    "print(\"gamma - gammaSD\", gamma - gammaSD)\n",
    "print(\"zbar_2017\", zbar_2017)\n",
    "print(\"theta\", theta)\n",
    "\n",
    "gammadata = pd.DataFrame(gamma)\n",
    "\n",
    "gammadata.to_csv('GammaData.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "518ca24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_density_function(gamma_val,\n",
    "                         gamma_vals_mean,\n",
    "                         theta_vals,\n",
    "                         site_precisions,\n",
    "                         alpha,\n",
    "                         X,\n",
    "                         Ua,\n",
    "                         Up,\n",
    "                         zbar_2017,\n",
    "                         forestArea_2017_ha,\n",
    "                         norm_fac,\n",
    "                         alpha_p_Adym,\n",
    "                         Bdym,\n",
    "                         leng,\n",
    "                         T,\n",
    "                         ds_vect,\n",
    "                         zeta,\n",
    "                         xi,\n",
    "                         kappa,\n",
    "                         pa,\n",
    "                         pf,\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Define a function to evaluate log-density of the objective/posterior distribution\n",
    "    Some of the input parameters are updated at each cycle of the outer loop (optimization loop),\n",
    "    and it becomes then easier/cheaper to udpate the function stamp and keep it separate here\n",
    "    \"\"\"\n",
    "    N          = X.shape[1] - 1\n",
    "    \n",
    "    gamma_val  = np.asarray(gamma_val).flatten()\n",
    "    gamma_size = gamma_val.size\n",
    "    x0_vals    = gamma_val.T.dot(forestArea_2017_ha) / norm_fac\n",
    "    X_zero     = np.sum(x0_vals) * np.ones(leng)\n",
    "    \n",
    "    \n",
    "    # shifted_X = zbar_2017 -  (X)[0:gamma_size, :-1]\n",
    "    shifted_X  = (X)[0: gamma_size, :-1].copy()\n",
    "    for j in range(N): \n",
    "        shifted_X[:, j]  = zbar_2017 - shifted_X[:, j]\n",
    "    omega      = np.dot(gamma_val, alpha * shifted_X -  (Up)[0:-1].T)\n",
    "    \n",
    "    X_dym      = np.zeros(T+1)\n",
    "    X_dym[0]   = np.sum(x0_vals)\n",
    "    X_dym[1: ] = alpha_p_Adym * X_zero  + np.dot(Bdym, omega.T)\n",
    "\n",
    "    z_shifted_X =  (X)[0: gamma_size, :].copy()\n",
    "    scl = pa * theta_vals - pf * kappa\n",
    "    for j in range(N+1): z_shifted_X [:, j] *= scl\n",
    "    term_1 = - casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * (Ua)[0:-1].T * zeta / 2 )\n",
    "    term_2 = casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * pf * (X_dym[1: ] - X_dym[0: -1]))\n",
    "    term_3 = casadi.sum2(np.reshape(ds_vect, (1, N+1)) * casadi.sum1(z_shifted_X))\n",
    "    obj_val = (term_1 + term_2 + term_3)\n",
    "\n",
    "    gamma_val_dev   = gamma_val - gamma_vals_mean\n",
    "    norm_log_prob   =   - 0.5 * np.dot(gamma_val_dev,\n",
    "                                       site_precisions.dot(gamma_val_dev)\n",
    "                                       )\n",
    "    log_density_val = -1.0  / xi * obj_val + norm_log_prob\n",
    "    if _DEBUG:\n",
    "        print(\"Term 1: \", term_1)\n",
    "        print(\"Term 2: \", term_2)\n",
    "        print(\"Term 3: \", term_3)\n",
    "        print(\"obj_val: \", obj_val)\n",
    "        print(\"norm_log_prob\", norm_log_prob)\n",
    "        print(\"log_density_val\", log_density_val)\n",
    "        print(\"gamma_value\", gamma_val)\n",
    "\n",
    "    return log_density_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a21fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    # Configurations/Settings\n",
    "    norm_fac          = 1e6,\n",
    "    delta_t           = 0.02,\n",
    "    alpha             = 0.045007414,\n",
    "    kappa             = 2.094215255,\n",
    "    pf                = 20.76,\n",
    "    pa                = 44.75,\n",
    "    xi                = 10,\n",
    "    zeta              = 1.66e-4*1e6,# zeta := 1.66e-4*norm_fac  #\n",
    "    #\n",
    "    max_iter          = 200,\n",
    "    tol               = 0.01,\n",
    "    T                 = 200,\n",
    "    N                 = 200,\n",
    "    #\n",
    "    sample_size       = 1000,    # simulations before convergence (to evaluate the mean)\n",
    "    mode_as_solution  = False,   # If true, use the mode (point of high probability) as solution for gamma\n",
    "    final_sample_size = 100_00, # number of samples to collect after convergence\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Main function; putting things together\n",
    "\n",
    "    :param float tol: convergence tolerance\n",
    "    :param T:\n",
    "    :param N:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Load sites' data\n",
    "    zbar_2017, gamma, gammaSD, \\\n",
    "        z_2017, forestArea_2017_ha, theta \\\n",
    "        = load_site_data(site_num,norm_fac=norm_fac)\n",
    "\n",
    "\n",
    "    # Evaluate Gamma values ()\n",
    "    gamma_1_vals  = gamma -  gammaSD\n",
    "    gamma_2_vals  = gamma +  gammaSD\n",
    "    gamma_size    = gamma.size\n",
    "\n",
    "    # Evaluate mean and covariances from site data\n",
    "    site_stdev       = gammaSD\n",
    "    site_covariances = np.diag(np.power(site_stdev, 2))\n",
    "    site_precisions  = np.linalg.inv(site_covariances)\n",
    "    site_mean        = gamma_1_vals/2 + gamma_2_vals/2\n",
    "\n",
    "    # Retrieve z data for selected site(s)\n",
    "    site_z_vals  = z_2017\n",
    "\n",
    "    # Initialize Gamma Values\n",
    "    gamma_vals      = gamma.copy()\n",
    "    gamma_vals_mean = gamma.copy()\n",
    "    gamma_vals_old  = gamma.copy()\n",
    "\n",
    "    # Theta Values\n",
    "    theta_vals  = theta\n",
    "\n",
    "    # Householder to track sampled gamma values\n",
    "    # gamma_vals_tracker       = np.empty((gamma_vals.size, sample_size+1))\n",
    "    # gamma_vals_tracker[:, 0] = gamma_vals.copy()\n",
    "    gamma_vals_tracker = [gamma_vals.copy()]\n",
    "\n",
    "    # Collected Ensembles over all iterations; dictionary indexed by iteration number\n",
    "    collected_ensembles = {}\n",
    "\n",
    "    # Track error over iterations\n",
    "    error_tracker = []\n",
    "\n",
    "    # Update this parameter (leng) once figured out where it is coming from\n",
    "    leng = 200\n",
    "    arr  = np.cumsum(\n",
    "             np.triu(\n",
    "             np.ones((leng, leng))\n",
    "         ),\n",
    "         axis=1,\n",
    "    ).T\n",
    "    Bdym         = (1-alpha) ** (arr-1)\n",
    "    Bdym[Bdym>1] = 0.0\n",
    "    Adym         = np.arange(1, leng+1)\n",
    "    alpha_p_Adym = np.power(1-alpha, Adym)\n",
    "\n",
    "    # Initialize Blocks of the A matrix those won't change\n",
    "    A  = np.zeros((gamma_size+2, gamma_size+2))\n",
    "    Ax = np.zeros(gamma_size+2)\n",
    "\n",
    "    # Construct Matrix B\n",
    "    B = np.eye(N=gamma_size+2, M=gamma_size, k=0)\n",
    "    B = casadi.sparsify(B)\n",
    "\n",
    "    # Construct Matrxi D constant blocks\n",
    "    D  = np.zeros((gamma_size+2, gamma_size))\n",
    "\n",
    "    # time step!\n",
    "    dt = T / N\n",
    "\n",
    "    # Other placeholders!\n",
    "    ds_vect = np.exp(- delta_t * np.arange(N+1) * dt)\n",
    "    ds_vect = np.reshape(ds_vect, (ds_vect.size, 1))\n",
    "\n",
    "    # Results dictionary\n",
    "    results = dict(\n",
    "        gamma_size=gamma_size,\n",
    "        tol=tol,\n",
    "        T=T,\n",
    "        N=N,\n",
    "        norm_fac=norm_fac,\n",
    "        delta_t=delta_t,\n",
    "        alpha=alpha,\n",
    "        kappa=kappa,\n",
    "        pf=pf,\n",
    "        pa=pa,\n",
    "        xi=xi,\n",
    "        zeta=zeta,\n",
    "        sample_size=sample_size,\n",
    "        final_sample_size=final_sample_size,\n",
    "        mode_as_solution=mode_as_solution,\n",
    "    )\n",
    "\n",
    "    # Initialize error & iteration counter\n",
    "    error = np.infty\n",
    "    cntr = 0\n",
    "\n",
    "    # Loop until convergence\n",
    "    while cntr < max_iter and error > tol:\n",
    "\n",
    "        # Update x0\n",
    "        x0_vals = gamma_vals * forestArea_2017_ha\n",
    "        \n",
    "        x0data = pd.DataFrame(x0_vals)\n",
    "        x0data.to_csv('X0Data.csv')\n",
    "        \n",
    "        gammadata = pd.DataFrame(gamma_vals)\n",
    "\n",
    "        gammadata.to_csv('GammaData.csv')\n",
    "        \n",
    "        cwd = os.getcwd() # get current working directory\n",
    "\n",
    "        ws = GamsWorkspace(system_directory=r\"C:\\GAMS\\43\", working_directory=cwd)\n",
    "        t1 = ws.add_job_from_file(\"amazon_25sites.gms\")\n",
    "        t1.run()\n",
    "        dfu = pd.read_csv('amazon_data_u.dat', delimiter='\\t')\n",
    "        # Process the data using the pandas DataFrame\n",
    "        dfu=dfu.drop('T/R ', axis=1)\n",
    "        dfu_np =dfu.to_numpy()\n",
    "\n",
    "        dfv = pd.read_csv('amazon_data_v.dat', delimiter='\\t')\n",
    "        # Process the data using the pandas DataFrame\n",
    "        dfv=dfv.drop('T/R ', axis=1)\n",
    "        dfv_np =dfv.to_numpy()\n",
    "\n",
    "        dfw = pd.read_csv('amazon_data_w.dat', delimiter='\\t')\n",
    "        # Process the data using the pandas DataFrame\n",
    "        dfw =dfw.drop('T   ', axis=1)\n",
    "        dfw_np = dfw.to_numpy()\n",
    "\n",
    "        dfx = pd.read_csv('amazon_data_x.dat', delimiter='\\t')\n",
    "        # Process the data using the pandas DataFrame\n",
    "        dfx =dfx.drop('T   ', axis=1)\n",
    "        dfx_np = dfx.to_numpy()\n",
    "\n",
    "        dfz = pd.read_csv('amazon_data_z.dat', delimiter='\\t')\n",
    "        # Process the data using the pandas DataFrame\n",
    "        dfz=dfz.drop('T/R ', axis=1)\n",
    "        dfz_np =dfz.to_numpy()\n",
    "\n",
    "        Ua = dfw_np**2\n",
    "        X_value = np.concatenate((dfz_np.T, dfx_np.T))\n",
    "        \"\"\"         if _DEBUG:\n",
    "            print(\"site_z_vals: \", site_z_vals)\n",
    "            print(\"x0_vals: \", x0_vals)\n",
    "            print(\"casadi.vertcat(site_z_vals,np.sum(x0_vals),1): \", casadi.vertcat(site_z_vals,np.sum(x0_vals),1))\n",
    "\n",
    "        if _DEBUG:\n",
    "            print(\" (X)\",  (X_value))\n",
    "            print(\" (Ua)\",  (Ua))\n",
    "            print(\" (Up)\",  (dfu_np))\n",
    "            print(\" (Um)\",  (dfv_np)) \"\"\"\n",
    "        \n",
    "        ## Start Sampling\n",
    "        # Update signature of log density evaluator\n",
    "        print(log_density_function(gamma_val=gamma_vals,\n",
    "                                                             gamma_vals_mean=gamma_vals_mean,\n",
    "                                                             theta_vals=theta_vals,\n",
    "                                                             site_precisions=site_precisions,\n",
    "                                                             alpha=alpha,\n",
    "                                                             X=X_value,\n",
    "                                                             Ua=Ua,\n",
    "                                                             Up=dfu_np,\n",
    "                                                             zbar_2017=zbar_2017,\n",
    "                                                             forestArea_2017_ha=forestArea_2017_ha,\n",
    "                                                             norm_fac=norm_fac,\n",
    "                                                             alpha_p_Adym=alpha_p_Adym,\n",
    "                                                             Bdym=Bdym,\n",
    "                                                             leng=leng,\n",
    "                                                             T=T,\n",
    "                                                             ds_vect=ds_vect,\n",
    "                                                             zeta=zeta,\n",
    "                                                             xi=xi,\n",
    "                                                             kappa=kappa,\n",
    "                                                             pa=pa,\n",
    "                                                             pf=pf,\n",
    "                                                             ))\n",
    "        log_density = lambda gamma_val: log_density_function(gamma_val=gamma_val,\n",
    "                                                             gamma_vals_mean=gamma_vals_mean,\n",
    "                                                             theta_vals=theta_vals,\n",
    "                                                             site_precisions=site_precisions,\n",
    "                                                             alpha=alpha,\n",
    "                                                             X=X_value,\n",
    "                                                             Ua=Ua,\n",
    "                                                             Up=dfu_np,\n",
    "                                                             zbar_2017=zbar_2017,\n",
    "                                                             forestArea_2017_ha=forestArea_2017_ha,\n",
    "                                                             norm_fac=norm_fac,\n",
    "                                                             alpha_p_Adym=alpha_p_Adym,\n",
    "                                                             Bdym=Bdym,\n",
    "                                                             leng=leng,\n",
    "                                                             T=T,\n",
    "                                                             ds_vect=ds_vect,\n",
    "                                                             zeta=zeta,\n",
    "                                                             xi=xi,\n",
    "                                                             kappa=kappa,\n",
    "                                                             pa=pa,\n",
    "                                                             pf=pf,\n",
    "                                                             )\n",
    "\n",
    "        # Create MCMC sampler & sample, then calculate diagnostics\n",
    "        sampler = create_hmc_sampler(\n",
    "            size=gamma_size,\n",
    "            log_density=log_density,\n",
    "            #\n",
    "            burn_in=100,\n",
    "            mix_in=2,\n",
    "            symplectic_integrator='verlet',\n",
    "            symplectic_integrator_stepsize=1e-1,\n",
    "            symplectic_integrator_num_steps=3,\n",
    "            mass_matrix=1,\n",
    "            constraint_test=lambda x: True if np.all(x>=0) else False,\n",
    "        )\n",
    "        gamma_post_samples = sampler.sample(\n",
    "            sample_size=sample_size,\n",
    "            initial_state=gamma_vals,\n",
    "            verbose=True,\n",
    "        )\n",
    "        gamma_post_samples = np.asarray(gamma_post_samples)\n",
    "\n",
    "        # Update ensemble/tracker\n",
    "        collected_ensembles.update({cntr: gamma_post_samples.copy()})\n",
    "\n",
    "        # Update gamma value\n",
    "        weight     = 0.25  # <-- Not sure how this linear combination weighting helps!\n",
    "        if mode_as_solution:\n",
    "            raise NotImplementedError(\"We will consider this in the future; trace sampled points and keep track of objective values to pick one with highest prob. \")\n",
    "            \n",
    "        else:\n",
    "            gamma_vals = weight * np.mean(gamma_post_samples, axis=0 ) + (1-weight) * gamma_vals_old\n",
    "        gamma_vals_tracker.append(gamma_vals.copy())\n",
    "\n",
    "        # Evaluate error for convergence check\n",
    "        error = np.max(np.abs(gamma_vals_old-gamma_vals) / gamma_vals_old)\n",
    "        error_tracker.append(error)\n",
    "        print(f\"Iteration [{cntr+1:4d}]: Error = {error}\")\n",
    "\n",
    "        # Exchange gamma values (for future weighting/update & error evaluation)\n",
    "        gamma_vals_old = gamma_vals\n",
    "\n",
    "        # Increase the counter\n",
    "        cntr += 1\n",
    "\n",
    "        results.update({'cntr': cntr,\n",
    "                        'error_tracker':np.asarray(error_tracker),\n",
    "                        'gamma_vals_tracker': np.asarray(gamma_vals_tracker),\n",
    "                        'collected_ensembles':collected_ensembles,\n",
    "                        })\n",
    "        pickle.dump(results, open('results.pcl', 'wb'))\n",
    "        \n",
    "        # Extensive plotting for monitoring; not needed really!\n",
    "        if False:\n",
    "            plt.plot(gamma_vals_tracker[-2], label=r'Old $\\gamma$')\n",
    "            plt.plot(gamma_vals_tracker[-1], label=r'New $\\gamma$')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            for j in range(gamma_size):\n",
    "                plt.hist(gamma_post_samples[:, j], bins=50)\n",
    "                plt.title(f\"Iteration {cntr}; Site {j+1}\")\n",
    "                plt.show()\n",
    "    \n",
    "    print(\"Terminated. Sampling the final distribution\")\n",
    "    # Sample (densly) the final distribution\n",
    "    final_sample = sampler.sample(\n",
    "        sample_size=final_sample_size,\n",
    "        initial_state=gamma_vals,\n",
    "        verbose=True,\n",
    "    )\n",
    "    final_sample = np.asarray(final_sample)\n",
    "    results.update({'final_sample': final_sample})\n",
    "    pickle.dump(results, open('results.pcl', 'wb'))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a2566dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term 1:  -13888\n",
      "Term 2:  336923\n",
      "Term 3:  36342.7\n",
      "obj_val:  359378\n",
      "norm_log_prob -0.0\n",
      "log_density_val -35937.8\n",
      "gamma_value [516.88557127 488.95668786 535.7933372  614.38476229 697.34085963\n",
      " 594.38440765 540.18969668 502.09011583 586.13789618 622.92870153\n",
      " 468.53543538 216.29945459 736.1126944  644.37719596 568.13204882\n",
      " 618.56342492 570.93769461 376.7589174  806.29456683 649.96115287\n",
      " 577.10631912 455.99483831 366.09394675 337.21852297 287.82630347]\n",
      "-35937.8\n",
      "Term 1:  -13888\n",
      "Term 2:  -166.744\n",
      "Term 3:  36342.7\n",
      "obj_val:  22287.9\n",
      "norm_log_prob -40979.27459759252\n",
      "log_density_val -43208.1\n",
      "gamma_value [-0.59080136 -2.74391297 -0.23456452  1.56612809 -1.45449092 -2.90130273\n",
      "  1.00045065 -1.49990151 -1.97265516 -0.59450001 -1.0402721  -0.47671579\n",
      " -1.33422536  0.53294927  1.01114906  0.32467001  0.01194545 -1.50321202\n",
      "  0.28702056  1.61132918 -0.18771085  0.53818759  0.68358976  0.94849772\n",
      "  0.93073371]\n",
      "Term 1:  -13888\n",
      "Term 2:  301.332\n",
      "Term 3:  36342.7\n",
      "obj_val:  22756\n",
      "norm_log_prob -40937.618969434916\n",
      "log_density_val -43213.2\n",
      "gamma_value [-2.91408062e-01  1.41843777e+00 -9.92945406e-01  3.44880426e-01\n",
      "  1.06593229e-01  9.64766251e-01 -5.29512678e-01 -1.64947031e+00\n",
      " -2.10732873e+00 -1.57182304e+00  3.71961918e+00  7.38755024e-01\n",
      "  6.76885291e-01  2.58880899e-03 -7.95137846e-01  1.16312143e+00\n",
      "  1.86746259e+00  1.61778851e+00 -8.87844779e-01 -1.41307381e-01\n",
      " -6.26779249e-01 -1.73687281e+00  3.58454341e-01  1.22028802e+00\n",
      " -2.14345721e-01]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log_density_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Samuel Zhao\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\Samuel Zhao\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"c:\\Users\\Samuel Zhao\\Documents\\Github\\HMC_amazon_project2\\mcmc\\mcmc_sampling.py\", line 149, in __call__\n    fd = float(self.func(self.x+self.e) - self.func(self.x)) / self.fd_eps\n  File \"c:\\Users\\Samuel Zhao\\Documents\\Github\\HMC_amazon_project2\\mcmc\\mcmc_sampling.py\", line 142, in <lambda>\n    self.func   = lambda x: float(func(x))\n  File \"C:\\Users\\Samuel Zhao\\AppData\\Local\\Temp\\ipykernel_31852\\579246538.py\", line 203, in <lambda>\nNameError: name 'log_density_function' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m main()\n",
      "Cell \u001b[1;32mIn[41], line 227\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(norm_fac, delta_t, alpha, kappa, pf, pa, xi, zeta, max_iter, tol, T, N, sample_size, mode_as_solution, final_sample_size)\u001b[0m\n\u001b[0;32m    203\u001b[0m log_density \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m gamma_val: log_density_function(gamma_val\u001b[39m=\u001b[39mgamma_val,\n\u001b[0;32m    204\u001b[0m                                                      gamma_vals_mean\u001b[39m=\u001b[39mgamma_vals_mean,\n\u001b[0;32m    205\u001b[0m                                                      theta_vals\u001b[39m=\u001b[39mtheta_vals,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m                                                      pf\u001b[39m=\u001b[39mpf,\n\u001b[0;32m    224\u001b[0m                                                      )\n\u001b[0;32m    226\u001b[0m \u001b[39m# Create MCMC sampler & sample, then calculate diagnostics\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m sampler \u001b[39m=\u001b[39m create_hmc_sampler(\n\u001b[0;32m    228\u001b[0m     size\u001b[39m=\u001b[39;49mgamma_size,\n\u001b[0;32m    229\u001b[0m     log_density\u001b[39m=\u001b[39;49mlog_density,\n\u001b[0;32m    230\u001b[0m     \u001b[39m#\u001b[39;49;00m\n\u001b[0;32m    231\u001b[0m     burn_in\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m    232\u001b[0m     mix_in\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m    233\u001b[0m     symplectic_integrator\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mverlet\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    234\u001b[0m     symplectic_integrator_stepsize\u001b[39m=\u001b[39;49m\u001b[39m1e-1\u001b[39;49m,\n\u001b[0;32m    235\u001b[0m     symplectic_integrator_num_steps\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m    236\u001b[0m     mass_matrix\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    237\u001b[0m     constraint_test\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m np\u001b[39m.\u001b[39;49mall(x\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39melse\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    238\u001b[0m )\n\u001b[0;32m    239\u001b[0m gamma_post_samples \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39msample(\n\u001b[0;32m    240\u001b[0m     sample_size\u001b[39m=\u001b[39msample_size,\n\u001b[0;32m    241\u001b[0m     initial_state\u001b[39m=\u001b[39mgamma_vals,\n\u001b[0;32m    242\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    244\u001b[0m gamma_post_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(gamma_post_samples)\n",
      "File \u001b[1;32mc:\\Users\\Samuel Zhao\\Documents\\Github\\HMC_amazon_project2\\mcmc\\mcmc_sampling.py:1198\u001b[0m, in \u001b[0;36mcreate_hmc_sampler\u001b[1;34m(size, log_density, log_density_grad, burn_in, mix_in, symplectic_integrator, symplectic_integrator_stepsize, symplectic_integrator_num_steps, mass_matrix, constraint_test)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m \u001b[39mGiven the size of the target space, and a function to evalute log density,\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39m    create and return an :py:class:`HMCSampler` instance/object to generate samples using HMC sampling approach.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[39mThis function shows how to create :py:class:`HMCSampler` instances (with some or all configurations passed)\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m configs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   1187\u001b[0m     size\u001b[39m=\u001b[39msize,\n\u001b[0;32m   1188\u001b[0m     log_density\u001b[39m=\u001b[39mlog_density,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     constraint_test\u001b[39m=\u001b[39mconstraint_test,\n\u001b[0;32m   1197\u001b[0m )\n\u001b[1;32m-> 1198\u001b[0m \u001b[39mreturn\u001b[39;00m HMCSampler(configs)\n",
      "File \u001b[1;32mc:\\Users\\Samuel Zhao\\Documents\\Github\\HMC_amazon_project2\\mcmc\\mcmc_sampling.py:213\u001b[0m, in \u001b[0;36mHMCSampler.__init__\u001b[1;34m(self, configs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_mass_matrix()\n\u001b[0;32m    212\u001b[0m \u001b[39m# Define log-density and associated gradient\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_log_density()\n\u001b[0;32m    215\u001b[0m \u001b[39m# maintain a proper random state\u001b[39;00m\n\u001b[0;32m    216\u001b[0m random_seed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_CONFIGURATIONS[\u001b[39m'\u001b[39m\u001b[39mrandom_seed\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Samuel Zhao\\Documents\\Github\\HMC_amazon_project2\\mcmc\\mcmc_sampling.py:473\u001b[0m, in \u001b[0;36mHMCSampler._update_log_density\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    470\u001b[0m test_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(size)\n\u001b[0;32m    472\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 473\u001b[0m grad \u001b[39m=\u001b[39m log_density_grad_parallel(test_vec)\n\u001b[0;32m    474\u001b[0m \u001b[39m# TODO: Remove after debugging\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParallel gradient took: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_time\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Samuel Zhao\\Documents\\Github\\HMC_amazon_project2\\mcmc\\mcmc_sampling.py:465\u001b[0m, in \u001b[0;36mHMCSampler._update_log_density.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m log_density_grad \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     log_density_grad_serial   \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__create_func_grad(log_density, size\u001b[39m=\u001b[39msize)\n\u001b[1;32m--> 465\u001b[0m     log_density_grad_parallel \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__parallel_func_grad(x, processes\u001b[39m=\u001b[39;49m\u001b[39mmin\u001b[39;49m(size, multiprocess\u001b[39m.\u001b[39;49mcpu_count()))\n\u001b[0;32m    466\u001b[0m     \u001b[39m# log_density_grad = lambda x: self.__threaded_func_grad(x, processes=min(size, multiprocess.cpu_count()))\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[39m# log_density_grad = self.__create_threaded_func_grad(processes=min(size, multiprocess.cpu_count()))\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \n\u001b[0;32m    469\u001b[0m     \u001b[39m# Test serial gradient\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     test_vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(size)\n",
      "File \u001b[1;32mc:\\Users\\Samuel Zhao\\Documents\\Github\\HMC_amazon_project2\\mcmc\\mcmc_sampling.py:366\u001b[0m, in \u001b[0;36mHMCSampler.__parallel_func_grad\u001b[1;34m(self, x, processes)\u001b[0m\n\u001b[0;32m    364\u001b[0m evaluator \u001b[39m=\u001b[39m FDGradient(func\u001b[39m=\u001b[39mfunc, x\u001b[39m=\u001b[39mx,)\n\u001b[0;32m    365\u001b[0m \u001b[39mwith\u001b[39;00m multiprocess\u001b[39m.\u001b[39mPool(processes) \u001b[39mas\u001b[39;00m pool:\n\u001b[1;32m--> 366\u001b[0m     grad \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(evaluator, \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(x)))\n\u001b[0;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(grad)\n",
      "File \u001b[1;32mc:\\Users\\Samuel Zhao\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    363\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[1;32mc:\\Users\\Samuel Zhao\\anaconda3\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_density_function' is not defined"
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18c79f95",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5016be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Plot Error Results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mplot(results[\u001b[39m'\u001b[39m\u001b[39merror_tracker\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mIteration\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot Error Results\n",
    "plt.plot(results['error_tracker'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gamma Estimate Update\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.plot(results['gamma_vals_tracker'][:, j], label=r\"$\\gamma_{%d}$\"%(j+1))\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbf2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms\n",
    "for itr in results['collected_ensembles'].keys():\n",
    "    for j in range(results['gamma_size']):\n",
    "        plt.hist(results['collected_ensembles'][itr][:, j], bins=100)\n",
    "        plt.title(f\"Iteration {itr+1}; Site {j+1}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histogram of the final sample\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.hist(results['final_sample'][:, j], bins=100)\n",
    "    plt.title(f\"Final Sample; Site {j+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3be0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
