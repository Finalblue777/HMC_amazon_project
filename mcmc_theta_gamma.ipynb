{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Import Required Packages\n",
    "# ========================\n",
    "import os, sys\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import casadi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "# os.chdir('/Users/sienzhao/Documents/amazon_mc')\n",
    "\n",
    "sys.path.append(os.path.abspath(\"mcmc\"))\n",
    "from mcmc_sampling import create_hmc_sampler\n",
    "    \n",
    "# Data Hanlder (.data_handlers.load_site_data)\n",
    "from data_handlers import load_site_data\n",
    "\n",
    "# Local Debugging flag; remove when all tested\n",
    "_DEBUG = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518ca24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_density_function(uparam_vals,\n",
    "                         uparam_vals_mean,\n",
    "                         site_precisions,\n",
    "                         alpha,\n",
    "                         sol,\n",
    "                         X,\n",
    "                         Ua,\n",
    "                         Up,\n",
    "                         zbar_2017_25Sites,\n",
    "                         forestArea_2017_ha_25Sites,\n",
    "                         norm_fac,\n",
    "                         alpha_p_Adym,\n",
    "                         Bdym,\n",
    "                         leng,\n",
    "                         T,\n",
    "                         ds_vect,\n",
    "                         zeta,\n",
    "                         xi,\n",
    "                         kappa,\n",
    "                         pa,\n",
    "                         pf,\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Define a function to evaluate log-density of the objective/posterior distribution\n",
    "    Some of the input parameters are updated at each cycle of the outer loop (optimization loop),\n",
    "    and it becomes then easier/cheaper to udpate the function stamp and keep it separate here\n",
    "    \"\"\"\n",
    "    N          = X.shape[1] - 1\n",
    "    uparam_vals = np.asarray(uparam_vals).flatten()\n",
    "    gamma_val  = np.asarray(uparam_vals[0:25]).flatten()\n",
    "    gamma_size = gamma_val.size\n",
    "    x0_vals    = gamma_val.T.dot(forestArea_2017_ha_25Sites) / norm_fac\n",
    "    X_zero     = np.sum(x0_vals) * np.ones(leng)\n",
    "    \n",
    "    \n",
    "    # shifted_X = zbar_2017_25Sites - sol.value(X)[0:gamma_size, :-1]\n",
    "    shifted_X  = sol.value(X)[0: gamma_size, :-1].copy()\n",
    "    for j in range(N): \n",
    "        shifted_X[:, j]  = zbar_2017_25Sites - shifted_X[:, j]\n",
    "    omega      = np.dot(gamma_val, alpha * shifted_X - sol.value(Up))\n",
    "    \n",
    "    X_dym      = np.zeros(T+1)\n",
    "    X_dym[0]   = np.sum(x0_vals)\n",
    "    X_dym[1: ] = alpha_p_Adym * X_zero  + np.dot(Bdym, omega.T)\n",
    "\n",
    "    z_shifted_X = sol.value(X)[0: gamma_size, :].copy()\n",
    "    theta_vals = uparam_vals[25:]\n",
    "\n",
    "    scl = pa * theta_vals - pf * kappa\n",
    "    for j in range(N+1): z_shifted_X [:, j] *= scl\n",
    "    \n",
    "    term_1 = - casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * sol.value(Ua) * zeta / 2 )\n",
    "    term_2 = casadi.sum2(np.reshape(ds_vect[0: T], (1, T)) * pf * (X_dym[1: ] - X_dym[0: -1]))\n",
    "    term_3 = casadi.sum2(np.reshape(ds_vect, (1, N+1)) * casadi.sum1(z_shifted_X))\n",
    "    \n",
    "    obj_val = term_1 + term_2 + term_3\n",
    "\n",
    "    parameter_val_dev = uparam_vals - uparam_vals_mean \n",
    "    norm_log_prob   =   - 0.5 * np.dot(parameter_val_dev,\n",
    "                                       site_precisions.dot(parameter_val_dev)\n",
    "                                       )\n",
    "    log_density_val = -1.0  / xi * obj_val + norm_log_prob\n",
    "\n",
    "    if _DEBUG:\n",
    "        print(\"Term 1: \", term_1)\n",
    "        print(\"Term 2: \", term_2)\n",
    "        print(\"Term 3: \", term_3)\n",
    "        print(\"obj_val: \", obj_val)\n",
    "        print(\"norm_log_prob\", norm_log_prob)\n",
    "        print(\"log_density_val\", log_density_val)\n",
    "\n",
    "    return log_density_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a21fbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    # Configurations/Settings\n",
    "    site_num          = 25,  # Number of sites(10, 25, 100, 1000)\n",
    "    norm_fac          = 1e9,\n",
    "    delta_t           = 0.02,\n",
    "    alpha             = 0.045007414,\n",
    "    kappa             = 2.094215255,\n",
    "    pf                = 20.76,\n",
    "    pa                = 44.75,\n",
    "    xi                = 15,\n",
    "    zeta              = 1.66e-4*1e9,  # zeta := 1.66e-4*norm_fac  #\n",
    "    #\n",
    "    max_iter          = 200,\n",
    "    tol               = 0.01,\n",
    "    T                 = 200,\n",
    "    N                 = 200,\n",
    "    #\n",
    "    sample_size       = 1000,    # simulations before convergence (to evaluate the mean)\n",
    "    mode_as_solution  = False,   # If true, use the mode (point of high probability) as solution for gamma\n",
    "    final_sample_size = 100_000, # number of samples to collect after convergence\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Main function; putting things together\n",
    "\n",
    "    :param float tol: convergence tolerance\n",
    "    :param T:\n",
    "    :param N:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Load sites' data\n",
    "    (\n",
    "        zbar_2017_25Sites, \n",
    "        gamma_25Sites, \n",
    "        gammaSD_25Sites, \n",
    "        z_2017_25Sites, \n",
    "        forestArea_2017_ha_25Sites, \n",
    "        theta_25Sites,\n",
    "        thetaSD_25Sites\n",
    "    ) = load_site_data(site_num, norm_fac=norm_fac)\n",
    "\n",
    "\n",
    "    # Evaluate Gamma values ()\n",
    "    gamma_1_vals  = gamma_25Sites -  gammaSD_25Sites\n",
    "    gamma_2_vals  = gamma_25Sites +  gammaSD_25Sites\n",
    "    gamma_size    = gamma_25Sites.size\n",
    "    \n",
    "    # Evaluate Theta values ()\n",
    "    theta_1_vals  = theta_25Sites -  thetaSD_25Sites\n",
    "    theta_2_vals  = theta_25Sites +  thetaSD_25Sites\n",
    "    theta_size    = theta_25Sites.size\n",
    "    \n",
    "    # Evaluate mean and covariances from site data\n",
    "    site_stdev       = np.concatenate((gammaSD_25Sites,thetaSD_25Sites))\n",
    "    site_covariances = np.diag(np.power(site_stdev, 2))\n",
    "    site_precisions  = np.linalg.inv(site_covariances)\n",
    "    site_mean        = np.concatenate((gamma_1_vals/2 + gamma_2_vals/2, theta_1_vals/2 + theta_2_vals/2))\n",
    "\n",
    "    # Retrieve z data for selected site(s)\n",
    "    site_z_vals  = z_2017_25Sites\n",
    "\n",
    "    # Initialize Gamma Values\n",
    "    gamma_vals      = gamma_25Sites.copy()\n",
    "    gamma_vals_mean = gamma_25Sites.copy()\n",
    "    gamma_vals_old  = gamma_25Sites.copy()\n",
    "\n",
    "    # Theta Values\n",
    "    theta_vals      = theta_25Sites.copy()\n",
    "    theta_vals_mean = theta_25Sites.copy()\n",
    "    theta_vals_old  = theta_25Sites.copy()\n",
    "    # Householder to track sampled gamma values\n",
    "    # gamma_vals_tracker       = np.empty((gamma_vals.size, sample_size+1))\n",
    "    # gamma_vals_tracker[:, 0] = gamma_vals.copy()\n",
    "    gamma_vals_tracker = [gamma_vals.copy()]\n",
    "    theta_vals_tracker = [theta_vals.copy()]\n",
    "\n",
    "    # Collected Ensembles over all iterations; dictionary indexed by iteration number\n",
    "    collected_ensembles = {}\n",
    "\n",
    "    # Track error over iterations\n",
    "    error_tracker = []\n",
    "\n",
    "    # Update this parameter (leng) once figured out where it is coming from\n",
    "    leng = 200\n",
    "    arr  = np.cumsum(\n",
    "             np.triu(\n",
    "             np.ones((leng, leng))\n",
    "         ),\n",
    "         axis=1,\n",
    "    ).T\n",
    "    Bdym         = (1-alpha) ** (arr-1)\n",
    "    Bdym[Bdym>1] = 0.0\n",
    "    Adym         = np.arange(1, leng+1)\n",
    "    alpha_p_Adym = np.power(1-alpha, Adym)\n",
    "\n",
    "    # Initialize Blocks of the A matrix those won't change\n",
    "    A  = np.zeros((gamma_size+2, gamma_size+2))\n",
    "    Ax = np.zeros(gamma_size+2)\n",
    "\n",
    "    # Construct Matrix B\n",
    "    B = np.eye(N=gamma_size+2, M=gamma_size, k=0)\n",
    "    B = casadi.sparsify(B)\n",
    "\n",
    "    # Construct Matrxi D constant blocks\n",
    "    D  = np.zeros((gamma_size+2, gamma_size))\n",
    "\n",
    "    # time step!\n",
    "    dt = T / N\n",
    "\n",
    "    # Other placeholders!\n",
    "    ds_vect = np.exp(- delta_t * np.arange(N+1) * dt)\n",
    "    ds_vect = np.reshape(ds_vect, (ds_vect.size, 1))\n",
    "\n",
    "    # Results dictionary\n",
    "    results = dict(\n",
    "        gamma_size = gamma_size,\n",
    "        theta_size = theta_size,\n",
    "        tol=tol,\n",
    "        T=T,\n",
    "        N=N,\n",
    "        norm_fac=norm_fac,\n",
    "        delta_t=delta_t,\n",
    "        alpha=alpha,\n",
    "        kappa=kappa,\n",
    "        pf=pf,\n",
    "        pa=pa,\n",
    "        xi=xi,\n",
    "        zeta=zeta,\n",
    "        sample_size=sample_size,\n",
    "        final_sample_size=final_sample_size,\n",
    "        mode_as_solution=mode_as_solution,\n",
    "    )\n",
    "\n",
    "    # Initialize error & iteration counter\n",
    "    error = np.infty\n",
    "    cntr = 0\n",
    "\n",
    "    # Loop until convergence\n",
    "    while cntr < max_iter and error > tol:\n",
    "\n",
    "        # Update x0\n",
    "        x0_vals = gamma_vals * forestArea_2017_ha_25Sites / norm_fac\n",
    "\n",
    "        # Construct Matrix A from new gamma_vals\n",
    "        A[: -2, :]        = 0.0\n",
    "        Ax[0: gamma_size] = - alpha * gamma_vals[0: gamma_size]\n",
    "        Ax[-1]            = alpha * np.sum(gamma_vals * zbar_2017_25Sites)\n",
    "        Ax[-2]            = - alpha\n",
    "        A[-2, :]          = Ax\n",
    "        A[-1, :]          = 0.0\n",
    "        A = casadi.sparsify(A)\n",
    "\n",
    "        # Construct Matrix D from new gamma_vals\n",
    "        D[:, :]  = 0.0\n",
    "        D[-2, :] = -gamma_vals\n",
    "        D = casadi.sparsify(D)\n",
    "        \n",
    "        # Define the right hand side (symbolic here) as a function of gamma\n",
    "        gamma = casadi.MX.sym('gamma' , gamma_size+2)\n",
    "        up    = casadi.MX.sym('up', gamma_size)\n",
    "        um    = casadi.MX.sym('um', gamma_size)\n",
    "\n",
    "        rhs = (A @ gamma + B @ (up-um) + D @ up) * dt + gamma\n",
    "        f = casadi.Function('f', [gamma, um, up], [rhs])\n",
    "        \n",
    "\n",
    "        ## Define an optimizer and initialize it, and set constraints\n",
    "        opti = casadi.Opti()\n",
    "\n",
    "        # Decision variables for states\n",
    "        X = opti.variable(gamma_size+2, N+1)\n",
    "\n",
    "        # Aliases for states\n",
    "        Up = opti.variable(gamma_size, N)\n",
    "        Um = opti.variable(gamma_size, N)\n",
    "        Ua = opti.variable(1, N)\n",
    "\n",
    "        # 1.2: Parameter for initial state\n",
    "        ic = opti.parameter(gamma_size+2)\n",
    "\n",
    "        # Gap-closing shooting constraints\n",
    "        for k in range(N):\n",
    "            opti.subject_to(X[:, k+1] == f(X[:, k], Um[:, k], Up[:, k]))\n",
    "\n",
    "        # Initial and terminal constraints\n",
    "        opti.subject_to(X[:, 0] == ic)\n",
    "        opti.subject_to(opti.bounded(0,\n",
    "                                     X[0: gamma_size, :],\n",
    "                                     zbar_2017_25Sites[0: gamma_size]\n",
    "                                     )\n",
    "                        )\n",
    "\n",
    "        # Objective: regularization of controls\n",
    "        for k in range(gamma_size):\n",
    "            opti.subject_to(opti.bounded(0, Um[k,:], casadi.inf))\n",
    "            opti.subject_to(opti.bounded(0, Up[k,:], casadi.inf))\n",
    "\n",
    "        opti.subject_to(Ua == casadi.sum1(Up+Um)**2)\n",
    "\n",
    "        # Set teh optimization problem\n",
    "        term1 = casadi.sum2(ds_vect[0: N, :].T * Ua * zeta / 2) \n",
    "        term2 = - casadi.sum2(ds_vect[0: N, :].T * (pf * (X[-2, 1: ] - X[-2, 0 :-1])))\n",
    "        term3 = - casadi.sum2(ds_vect.T * casadi.sum1( (pa * theta_vals - pf * kappa ) * X[0: gamma_size, :] ))\n",
    "        \n",
    "        opti.minimize(term1 + term2 + term3)\n",
    "\n",
    "        # Solve optimization problem\n",
    "        options               = dict()\n",
    "        options[\"print_time\"] = False\n",
    "        options[\"expand\"]     = True\n",
    "        options[\"ipopt\"]      = {'print_level':                      0,\n",
    "                                 'fast_step_computation':            'yes',\n",
    "                                 'mu_allow_fast_monotone_decrease':  'yes',\n",
    "                                 'warm_start_init_point':            'yes',\n",
    "                                 }\n",
    "        opti.solver('ipopt', options)\n",
    "        opti.set_value(ic,\n",
    "                       casadi.vertcat(site_z_vals,\n",
    "                                      np.sum(x0_vals),\n",
    "                                      1),\n",
    "                       )\n",
    "        if _DEBUG:\n",
    "            print(\"ic: \", ic)\n",
    "            print(\"site_z_vals: \", site_z_vals)\n",
    "            print(\"x0_vals: \", x0_vals)\n",
    "            print(\"casadi.vertcat(site_z_vals,np.sum(x0_vals),1): \", casadi.vertcat(site_z_vals,np.sum(x0_vals),1))\n",
    "        sol = opti.solve()\n",
    "\n",
    "        if _DEBUG:\n",
    "            print(\"sol.value(X)\", sol.value(X))\n",
    "            print(\"sol.value(Ua)\", sol.value(Ua))\n",
    "            print(\"sol.value(Up)\", sol.value(Up))\n",
    "            print(\"sol.value(Um)\", sol.value(Um))\n",
    "        \n",
    "        \n",
    "        ## Start Sampling\n",
    "        # Update signature of log density evaluator\n",
    "        log_density = lambda uparam_vals : log_density_function(uparam_vals = np.concatenate((gamma_vals,theta_vals)),\n",
    "                                                             uparam_vals_mean = np.concatenate((gamma_vals_mean,theta_vals_mean)),\n",
    "                                                             site_precisions=site_precisions,\n",
    "                                                             alpha=alpha,\n",
    "                                                             sol=sol,\n",
    "                                                             X=X,\n",
    "                                                             Ua=Ua,\n",
    "                                                             Up=Up,\n",
    "                                                             zbar_2017_25Sites=zbar_2017_25Sites,\n",
    "                                                             forestArea_2017_ha_25Sites=forestArea_2017_ha_25Sites,\n",
    "                                                             norm_fac=norm_fac,\n",
    "                                                             alpha_p_Adym=alpha_p_Adym,\n",
    "                                                             Bdym=Bdym,\n",
    "                                                             leng=leng,\n",
    "                                                             T=T,\n",
    "                                                             ds_vect=ds_vect,\n",
    "                                                             zeta=zeta,\n",
    "                                                             xi=xi,\n",
    "                                                             kappa=kappa,\n",
    "                                                             pa=pa,\n",
    "                                                             pf=pf,\n",
    "                                                             )\n",
    "\n",
    "        # Create MCMC sampler & sample, then calculate diagnostics\n",
    "        sampler = create_hmc_sampler(\n",
    "            size=gamma_size + theta_size,\n",
    "            log_density=log_density,\n",
    "            #\n",
    "            burn_in=100,\n",
    "            mix_in=2,\n",
    "            symplectic_integrator='verlet',\n",
    "            symplectic_integrator_stepsize=1e-1,\n",
    "            symplectic_integrator_num_steps=3,\n",
    "            mass_matrix=1e-3,\n",
    "            constraint_test=lambda x: True if np.all(x>=0) else False,\n",
    "        )\n",
    "        gamma_post_samples = sampler.sample(\n",
    "            sample_size=sample_size,\n",
    "            initial_state=np.concatenate((gamma_vals,theta_vals)),\n",
    "            verbose=True,\n",
    "        )\n",
    "        post_samples = np.asarray(post_samples)\n",
    "\n",
    "        # Update ensemble/tracker\n",
    "        collected_ensembles.update({cntr: post_samples.copy()})\n",
    "\n",
    "        # Update gamma value\n",
    "        weight     = 0.25  # <-- Not sure how this linear combination weighting helps!\n",
    "        if mode_as_solution:\n",
    "            raise NotImplementedError(\"We will consider this in the future; trace sampled points and keep track of objective values to pick one with highest prob. \")\n",
    "            \n",
    "        else:\n",
    "            gamma_vals = weight * np.mean(post_samples, axis=0 )[0:25] + (1-weight) * gamma_vals_old\n",
    "            theta_vals = weight * np.mean(post_samples, axis=0 )[25:]  + (1-weight) * theta_vals_old\n",
    "\n",
    "        gamma_vals_tracker.append(gamma_vals.copy())\n",
    "        theta_vals_tracker.append(theta_vals.copy())\n",
    "\n",
    "        # Evaluate error for convergence check\n",
    "        error = np.max(np.concatenate((np.abs(gamma_vals_old-gamma_vals) / gamma_vals_old),\n",
    "                       np.abs(theta_vals_old-theta_vals) / theta_vals_old ))\n",
    "        error_tracker.append(error)\n",
    "        print(f\"Iteration [{cntr+1:4d}]: Error = {error}\")\n",
    "\n",
    "        # Exchange gamma values (for future weighting/update & error evaluation)\n",
    "        gamma_vals_old = gamma_vals\n",
    "        theta_vals_old = theta_vals\n",
    "\n",
    "        # Increase the counter\n",
    "        cntr += 1\n",
    "\n",
    "        results.update({'cntr': cntr,\n",
    "                        'error_tracker':np.asarray(error_tracker),\n",
    "                        'gamma_vals_tracker': np.asarray(gamma_vals_tracker),\n",
    "                        'theta_vals_tracker': np.asarray(theta_vals_tracker),\n",
    "                        'collected_ensembles':collected_ensembles,\n",
    "                        })\n",
    "        pickle.dump(results, open('results.pcl', 'wb'))\n",
    "        \n",
    "        # Extensive plotting for monitoring; not needed really!\n",
    "        if False:\n",
    "            plt.plot(gamma_vals_tracker[-2], label=r'Old $\\gamma$')\n",
    "            plt.plot(gamma_vals_tracker[-1], label=r'New $\\gamma$')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.plot(theta_vals_tracker[-2], label=r'Old $\\theta$')\n",
    "            plt.plot(theta_vals_tracker[-1], label=r'New $\\theta$')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            for j in range(gamma_size):\n",
    "                plt.hist(post_samples[0:25, j], bins=50)\n",
    "                plt.title(f\"$\\gamma$ Iteration {cntr}; Site {j+1}\")\n",
    "                plt.show()\n",
    "                plt.hist(post_samples[25:, j], bins=50)\n",
    "                plt.title(f\"$\\theta$ Iteration {cntr}; Site {j+1}\")\n",
    "                plt.show()\n",
    "    \n",
    "    print(\"Terminated. Sampling the final distribution\")\n",
    "    # Sample (densly) the final distribution\n",
    "    final_sample = sampler.sample(\n",
    "        sample_size=final_sample_size,\n",
    "        initial_state=np.concatenate((gamma_vals,theta_vals)),\n",
    "        verbose=True,\n",
    "    )\n",
    "    final_sample = np.asarray(final_sample)\n",
    "    results.update({'final_sample': final_sample})\n",
    "    pickle.dump(results, open('results.pcl', 'wb'))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2566dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from '/Users/attia/AHMED_HOME/Research/Projects/Mappers/FundedProjects/SciDAC/FastMath/Research/Economics/Code/HMC_amazon_project/data/calibration_25SitesModel.csv'\n"
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c79f95",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5016be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error Results\n",
    "plt.plot(results['error_tracker'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gamma Estimate Update\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.plot(results['gamma_vals_tracker'][:, j], label=r\"$\\gamma_{%d}$\"%(j+1))\n",
    "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbf2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms\n",
    "for itr in results['collected_ensembles'].keys():\n",
    "    for j in range(results['gamma_size']):\n",
    "        plt.hist(results['collected_ensembles'][itr][:, j], bins=100)\n",
    "        plt.title(f\"Iteration {itr+1}; Site {j+1}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histogram of the final sample\n",
    "for j in range(results['gamma_size']):\n",
    "    plt.hist(results['final_sample'][:, j], bins=100)\n",
    "    plt.title(f\"Final Sample; Site {j+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3be0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
